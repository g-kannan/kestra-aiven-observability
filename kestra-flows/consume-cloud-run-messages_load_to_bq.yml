id: consume-cloud-run-messages_load_to_bq
namespace: aiven

tasks:
  - id: consume
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
      image: python:3.11-slim
    beforeCommands:
      - pip install kafka-python
    inputFiles:
      ca.pem: "{{ read('certs/ca.pem') }}"
      service.cert: "{{ read('certs/service.cert') }}"
      service.key: "{{ read('certs/service.key') }}"
    outputFiles:
      - messages.jsonl
    script: |
      import json
      from kafka import KafkaConsumer

      # Consume messages using kafka-python with SSL (uses PEM files directly!)
      consumer = KafkaConsumer(
          'gcp-cloud-run-fn-events',
          bootstrap_servers='kafka-15145f3d-project-4683.g.aivencloud.com:11353',
          security_protocol='SSL',
          ssl_cafile='ca.pem',
          ssl_certfile='service.cert',
          ssl_keyfile='service.key',
          auto_offset_reset='earliest',
          enable_auto_commit=False,
          group_id='kestra-cloudrun-event-consumer',
          consumer_timeout_ms=20000,  # 20 seconds timeout
          value_deserializer=lambda x: json.loads(x.decode('utf-8')) if x else None
      )

      messages = []
      count = 0
      with open('messages.jsonl', 'w') as f:
          for message in consumer:
              # Wrap the message value into a single 'value' column
              # We serialize it back to JSON string to fit in a STRING column
              row = {
                  'value': json.dumps(message.value)
              }
              f.write(json.dumps(row) + '\n')
              count += 1
              if count >= 1000:
                  break

      consumer.close()
      print(f"Formatted {count} messages for BigQuery")

  - id: load_to_bq
    type: io.kestra.plugin.gcp.bigquery.Load
    serviceAccount: "{{ read('certs/service-account.json') }}"
    from: "{{ outputs.consume.outputFiles['messages.jsonl'] }}"
    destinationTable: "{{ kv('bq_cr_event_tbl') }}"
    format: JSON
    writeDisposition: WRITE_APPEND
    schema:
      fields:
        - name: value
          type: STRING